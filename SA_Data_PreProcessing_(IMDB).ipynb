{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobTumak/SentimentAnalysisProject/blob/main/SA_Data_PreProcessing_(IMDB).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejv17dQs_RZx"
      },
      "source": [
        "#**Importing/Installing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8czLxQF_NBv",
        "outputId": "9f5859a7-ed0d-4434-e29c-1d0b13940570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk import (sent_tokenize, word_tokenize)\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(['punkt', 'stopwords'])\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny93Gqp61byz"
      },
      "outputs": [],
      "source": [
        "\n",
        "stop_words.add('br')\n",
        "# 'br' appears most in text later on as a break operator.\n",
        "# Adding it to the stopwords list now willprevent it from \n",
        "# becoming the most seen word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmM5-mXF8i9R"
      },
      "source": [
        "# **Importing DataSets from google Drive**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anZSaMk681A9"
      },
      "source": [
        "Read the file taken from drive into a list of reviews. Drive must be mounted into the same drive account that colab is using or else it won't mount."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxABm1dY7okA",
        "outputId": "7cc362da-a668-4122-8c70-71ce3dd57986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive # Must use same account as the notebook is in, otherwise it won't mount\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXUPmbns3wW-"
      },
      "outputs": [],
      "source": [
        "def fetch_data(identifier, min_index, max_index): # Uses \"append\" method\n",
        "  output_var = []\n",
        "  for i in range(min_index, max_index + 1):\n",
        "    for j in range(-1,11):\n",
        "      try:\n",
        "        output_var.append((open(f'/content/gdrive/My Drive/Train/{identifier}/{i}_{j}.txt', 'r')).read())\n",
        "      except:\n",
        "        continue\n",
        "  return output_var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS4svOFS6yqh"
      },
      "source": [
        "compile first 10 positive and negative reviews into their respective lists. Just for testing. Final trial will have more reviews loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYftZeUcAh2L"
      },
      "outputs": [],
      "source": [
        "data_test_pos = fetch_data('pos', 0,10)\n",
        "data_test_neg = fetch_data('neg', 0, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u15brxHdGcq9"
      },
      "source": [
        "# **Tokenizing Raw-Data into word-lists**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0skK-BhVR4W"
      },
      "outputs": [],
      "source": [
        "def process_data(data_set):\n",
        "  \n",
        "  data_set = [word_tokenize(data) for data in data_set] # seperates strings into lists of words in original order\n",
        "  data_set = [data_set[i][j] for i in range(len(data_set)) for j in range(len(data_set[i])) if data_set[i][j].lower() not in stop_words and data_set[i][j].isalpha()]\n",
        "  [\"\".join(word.lower()) for word in data_set]\n",
        "  return data_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJMhdmSXYojs"
      },
      "outputs": [],
      "source": [
        "filtered_pos = process_data(data_test_pos)\n",
        "filtered_neg = process_data(data_test_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVwlCA3mkO3C"
      },
      "source": [
        "Make a function that produces a dictionary with the words from the word-list data as keywords and the frequency of use as values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgqeB49rjNea"
      },
      "outputs": [],
      "source": [
        "def word_frequency(list1, list2):\n",
        "\n",
        "  all_words = list1 + list2\n",
        "  top_words = {word : 0 for word in all_words}\n",
        "  \n",
        "  for word in list1:\n",
        "    top_words[word] += 1\n",
        "\n",
        "  # Previous-clunkier algorithm:\n",
        "  # top_words = {}\n",
        "  # for word in list1:\n",
        "  #   if word in top_words.keys():\n",
        "  #     top_words[word] += 1\n",
        "  #   else:\n",
        "  #     top_words[word] = 1\n",
        "  #   for word in list2:\n",
        "  #     if word not in list1:\n",
        "  #       top_words[word] = 0\n",
        "        \n",
        "  return dict(sorted(top_words.items(), key=lambda x:x[1], reverse=True))\n",
        "  # return top_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hytYC-uy0zH-"
      },
      "source": [
        "Compile all negative and positive words into dictionary to compare most keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1RZvOFLCurb"
      },
      "outputs": [],
      "source": [
        "neg_words = word_frequency(filtered_neg, filtered_pos)\n",
        "pos_words = word_frequency(filtered_pos, filtered_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djkx7X4v1p5h"
      },
      "source": [
        "Now to find most common words used in both sets of reviews (pos and neg) and add filter them out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqljRKof2UoG"
      },
      "outputs": [],
      "source": [
        "# This was my first attempt at structuring the data\n",
        "# first_try = [{word : ({'Positive':[{'Int':pos_words[word]}, {'Dist':float(pos_words[word]/(pos_words[word]+neg_words[word]))}]}, {'Negative':[{'Int':neg_words[word]}, {'Dist':float(neg_words[word])/(pos_words[word]+neg_words[word])}]})} for word in pos_words if word in neg_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_oI1obo_avT"
      },
      "outputs": [],
      "source": [
        "all_word_data = {word : {'Positive':{'Int':pos_words[word], 'Dist':round(float(pos_words[word]/(pos_words[word]+neg_words[word])), 2)}, 'Negative':{'Int':neg_words[word], 'Dist':round(float(neg_words[word]/(pos_words[word]+neg_words[word])), 2)}} for word in pos_words if word in neg_words}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQLdaF7i4C2i"
      },
      "source": [
        "Creates dataset of common word data as *common_word_data[word]['Positive' or 'Negative']['Int' or 'Dist']*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EcLulLJJo3b"
      },
      "source": [
        "#**Compiling and Exporting Data to use in statistical-based sentiment analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmym_rbuJ_Do"
      },
      "outputs": [],
      "source": [
        "# with open(\"/content/gdrive/My Drive/Train/test2_file.txt\", \"w\") as test_file:\n",
        "#      test_file.write(json.dumps(all_word_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1_4TNj3Ib_G"
      },
      "source": [
        "#**adverb and adjective based analysis trial**\n",
        "The previous method of processing data develops a machine learning model based on word use analysis. \n",
        "\n",
        "My aim in this following section is to develop a system to do sentence based analysis rather than based solely on words.\n",
        "My first idea is to compile lists of adjectives and positive-and-negative adverbs (modifier words) and search for them sentence by sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-91c9dAInP1",
        "outputId": "63d25263-bc8c-4f95-ad9c-e3f4a061ce78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a7e9a5c07c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Getting adjectives and adverbs stored in my drive on google sheets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgspread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output)\u001b[0m\n\u001b[1;32m    228\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_check_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CredentialType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_auth_ephem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m       _message.blocking_request(\n\u001b[0m\u001b[1;32m    231\u001b[0m           \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m           \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'auth_user_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Getting adjectives and adverbs stored in my drive on google sheets\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "worksheet = gc.open('Trial2Spreadsheet').sheet1\n",
        "\n",
        "# Sorting sheets data into structured set for easier use later on\n",
        "rows = worksheet.get_all_values()\n",
        "data_set = dict()\n",
        "data_set['pos adj'] = [row[0] for row in rows if row[0] != \"\"]\n",
        "data_set['neg adj'] = [row[1] for row in rows if row[1] != \"\"]\n",
        "data_set['pos adv'] = [row[2] for row in rows if row[2] != \"\"]\n",
        "data_set['neg adv'] = [row[3] for row in rows if row[3] != \"\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading previously made data set from reviews (stored in my drive as a .txt file)\n",
        "with open(\"/content/gdrive/MyDrive/Train/test_file.txt\", 'r') as json_file:\n",
        "  data_set = json.load(json_file)"
      ],
      "metadata": {
        "id": "LoSrumaRltvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding and sorting all adjectives and adverbs into "
      ],
      "metadata": {
        "id": "u3hhC73HmM3M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "A1_4TNj3Ib_G"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPZdrTBSQtFAQ+oCf8qF8KG",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}